{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import json, os, glob, subprocess\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "id_to_name = defaultdict(str)\n",
    "doc_sentences = defaultdict(lambda: []) # each user (key) has a dict of sentences (value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "\n",
    "try:\n",
    "    project_dir = os.environ['PROJECT_PATH']\n",
    "except:\n",
    "    print(\"Please make sure you have set your PROJECT_PATH variable as described in the readme.\")\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of your transcript file: utilities/transcript-15528094.json\n"
     ]
    }
   ],
   "source": [
    "######## read_sentences.ipynb ########\n",
    "\n",
    "# read data from transcript\n",
    "transcript_filename = input(\"Enter the name of your transcript file: \")\n",
    "\n",
    "try:\n",
    "    transcript_filepath = os.path.join(project_dir, transcript_filename)\n",
    "    with open(transcript_filepath) as f:\n",
    "        data = json.load(f)\n",
    "except:\n",
    "    print(\"Could not open the file \" + str(transcript_filename) + \" in \" + os.path.split(transcript_filepath)[:-1][0])\n",
    "    exit(0)\n",
    "\n",
    "for item in data:\n",
    "    if item['text'] != None:\n",
    "        \n",
    "        user_id = item['user_id']\n",
    "        message = item['text']\n",
    "        \n",
    "        # ignore system and calendar messages\n",
    "        if user_id == 'system' or user_id == 'calendar':\n",
    "            continue\n",
    "        \n",
    "        # Split message if it contains newlines\n",
    "        sents = message.split('\\n')\n",
    "        \n",
    "        # separate message (or lines) into sentences\n",
    "        sents = [item for s in sents for item in re.split('(?<=[.!?]) +',s)]\n",
    "        # regex from https://stackoverflow.com/questions/14622835/split-string-on-or-keeping-the-punctuation-mark\n",
    "        \n",
    "        # add sentences to dict\n",
    "        for sentence in sents:\n",
    "            if len(sentence) == 0:\n",
    "                # skip any empty sentences\n",
    "                continue\n",
    "            doc_sentences[user_id].append(sentence)\n",
    "        \n",
    "        # potentially add user to id_to_name dict\n",
    "        if user_id not in id_to_name:\n",
    "            id_to_name[user_id] = item['name']\n",
    "            \n",
    "# utilities/transcript-15528094.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print each user's sentences to a different file\n",
    "# each sentence on a new line\n",
    "\n",
    "os.makedirs('sentences', exist_ok=True) # make the sentences directory if it doesn't already exist\n",
    "\n",
    "for user in doc_sentences:\n",
    "    filename = str(user) + '_sentences.txt'\n",
    "    # store them in their own directory\n",
    "    filepath = os.path.join('sentences', filename)\n",
    "    \n",
    "    outfile = open(filepath, \"w\")\n",
    "    \n",
    "    sentences = doc_sentences[user]\n",
    "    for sentence in sentences:\n",
    "        outfile.write(sentence + \"\\n\")\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### parse_sentences.ipynb #########\n",
    "\n",
    "# now do sentence complexity analysis\n",
    "\n",
    "# set parser environment vars\n",
    "os.environ['STANFORD_PARSER'] = '~/Downloads/stanford-parser-full-2018-10-17'\n",
    "os.environ['STANFORD_MODELS'] = '~/Downloads/stanford-parser-full-2018-10-17'\n",
    "os.environ['CLASSPATH'] = '~/Downloads/stanford-parser-full-2018-10-17/*'\n",
    "\n",
    "# start up CoreNLP server\n",
    "\n",
    "# start it as a background process:\n",
    "command = \"java -mx4g -cp \\\"*\\\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -preload tokenize,ssplit,pos,lemma,ner,parse,depparse -status_port 9000 -port 9000 -timeout 15000 &\"\n",
    "subprocess.Popen(command.split())\n",
    "\n",
    "# now connect to it\n",
    "parser = nltk.parse.corenlp.CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of tree height and count\n",
    "counts = defaultdict(lambda: [0,0]) # keep a tuple: total, count\n",
    "\n",
    "# to store averages\n",
    "average_depth = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = glob.glob('sentences/*_sentences.txt')\n",
    "parsed_path = 'parsed'\n",
    "os.makedirs(parsed_path, exist_ok=True) # make the parsed directory if it doesn't already exist\n",
    "\n",
    "for file in data_path:\n",
    "    \n",
    "    curr_file = open(file, \"r\")\n",
    "    filename = os.path.split(file)[-1:][0]\n",
    "    user = filename.split(\"_\")[0]\n",
    "    \n",
    "    # write parsed sentences to a file\n",
    "    # TODO: add functionality to read from file, or read only new stuff from the file\n",
    "    parsed_file = os.path.join(parsed_path, str(user) + '_parsed.txt')\n",
    "    outfile = open(parsed_file, 'w')\n",
    "\n",
    "    for line in curr_file:\n",
    "        \n",
    "        # skip lines that contain unicode\n",
    "        if any(ord(c) >= 128 for c in line):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            parsed = parser.parse(line.split())\n",
    "            print(parsed)\n",
    "            # from answer by alvas on https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk\n",
    "            # also here: https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\n",
    "        except:\n",
    "            #print(\"Could not parse this line: \" + str(line))\n",
    "            #print(\"Skipping...\")\n",
    "            continue\n",
    "\n",
    "        outfile.write(parsed)\n",
    "        \n",
    "        tree = next(parsed)\n",
    "        height = tree.height()\n",
    "        \n",
    "        # update height total\n",
    "        counts[user][0] += height\n",
    "        # update count\n",
    "        counts[user][1] += 1\n",
    "        \n",
    "    outfile.close()\n",
    "    \n",
    "    if(counts[user][1] == 0):\n",
    "        # avoid division by zero\n",
    "        continue\n",
    "    average_depth[user] = counts[user][0]/counts[user][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict of names for each id\n",
    "fakename = defaultdict(str)\n",
    "\n",
    "fakename_file = os.path.join(project_dir, 'utilities/fakenames.txt')\n",
    "try:\n",
    "    f = open(fakename_file, 'r')\n",
    "except:\n",
    "    print(\"Could not find fakenames.txt in the utilities directory of your project path.\")\n",
    "    exit(0)\n",
    "\n",
    "try:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        fakename[tokens[0]] = tokens[1]\n",
    "except:\n",
    "    print(\"The file utilities/fakenames.txt is improperly formatted.\")\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan & 6.833 \\\\\n",
      "\n",
      "length: 1\n"
     ]
    }
   ],
   "source": [
    "######## results.ipynb ###########\n",
    "\n",
    "results_file = open(\"results.txt\", 'w')\n",
    "results_latex = open(\"results_latex.txt\", 'w')\n",
    "\n",
    "# print sorted list in human-readable and latex table format\n",
    "for key_value_pair in sorted(average_depth.items(),\n",
    "           key=lambda k_v: k_v[1],\n",
    "           reverse=True):\n",
    "        \n",
    "        user = key_value_pair[0]\n",
    "        average = key_value_pair[1]\n",
    "        \n",
    "        results_latex.write(\"{} & {:.3f} \\\\\\\\\".format(fake_name[user], average))\n",
    "        results_file.write(fake_name[user] + \"\\t\" + str(average) + \"\\n\")\n",
    "\n",
    "print(\"Processed \" + str(len(average_depth)) + \"users. Exiting...\")\n",
    "\n",
    "results_latex.close()\n",
    "results_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
