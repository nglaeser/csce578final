{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import json, os, glob\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "id_to_name = defaultdict(str)\n",
    "doc_sentences = defaultdict(lambda: []) # each user (key) has a dict of sentences (value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## read_sentences.ipynb ########\n",
    "\n",
    "# read data from old transcript\n",
    "with open('transcript-15528094.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for item in data:\n",
    "    if item['text'] != None:\n",
    "        \n",
    "        user_id = item['user_id']\n",
    "        message = item['text']\n",
    "        \n",
    "        # ignore system and calendar messages\n",
    "        if user_id == 'system' or user_id == 'calendar':\n",
    "            continue\n",
    "        \n",
    "        # Split message if it contains newlines\n",
    "        sents = message.split('\\n')\n",
    "        \n",
    "        # separate message (or lines) into sentences\n",
    "        sents = [item for s in sents for item in re.split('(?<=[.!?]) +',s)]\n",
    "        # regex from https://stackoverflow.com/questions/14622835/split-string-on-or-keeping-the-punctuation-mark\n",
    "        \n",
    "        # add sentences to dict\n",
    "        for sentence in sents:\n",
    "            if len(sentence) == 0:\n",
    "                # skip any empty sentences\n",
    "                continue\n",
    "            doc_sentences[user_id].append(sentence)\n",
    "        \n",
    "        # potentially add user to id_to_name dict\n",
    "        if user_id not in id_to_name:\n",
    "            id_to_name[user_id] = item['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print each user's sentences to a different file\n",
    "# each sentence on a new line\n",
    "\n",
    "for user in doc_sentences:\n",
    "    filename = str(user) + '_sentences.txt'\n",
    "    outfile = open(filename, \"w\")\n",
    "    \n",
    "    sentences = doc_sentences[user]\n",
    "    for sentence in sentences:\n",
    "        outfile.write(sentence + \"\\n\")\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### parse_sentences.ipynb #########\n",
    "\n",
    "# now do sentence complexity analysis\n",
    "\n",
    "# set parser environment vars\n",
    "os.environ['STANFORD_PARSER'] = '~/Downloads/stanford-parser-full-2018-10-17'\n",
    "os.environ['STANFORD_MODELS'] = '~/Downloads/stanford-parser-full-2018-10-17'\n",
    "os.environ['CLASSPATH'] = '~/Downloads/stanford-parser-full-2018-10-17/*'\n",
    "\n",
    "# start up CoreNLP server\n",
    "parser = nltk.parse.corenlp.CoreNLPParser(url='http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of tree height and count\n",
    "counts = defaultdict(lambda: [0,0]) # keep a tuple: total, count\n",
    "\n",
    "# to store averages\n",
    "average_depth = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = glob.glob('*_sentences.txt')\n",
    "\n",
    "for filename in data_path:\n",
    "    \n",
    "    curr_file = open(filename, \"r\")\n",
    "    user = filename.split(\"_\")[0]\n",
    "\n",
    "    for line in curr_file:\n",
    "        \n",
    "        # skip lines that contain unicode\n",
    "        if any(ord(c) >= 128 for c in line):\n",
    "            continue\n",
    "        \n",
    "        # TODO: add try-catch here\n",
    "        parsed = parser.parse(line.split())\n",
    "        # from answer by alvas on https://stackoverflow.com/questions/13883277/stanford-parser-and-nltk\n",
    "        # also here: https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\n",
    "\n",
    "        tree = next(parsed)\n",
    "        # TODO: output parsed sentences into a file\n",
    "        height = tree.height()\n",
    "        \n",
    "        # update height total\n",
    "        counts[user][0] += height\n",
    "        # update count\n",
    "        counts[user][1] += 1\n",
    "        \n",
    "    average_depth[user] = counts[user][0]/counts[user][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict of names for each id\n",
    "fake_name = {'10820250': 'Jack',\n",
    "             '13207027': 'John',\n",
    "             '14137183': 'Peter',\n",
    "             '18559389': 'Anna',\n",
    "             '18559680': 'Ethan',\n",
    "             '19458971': 'Alex',\n",
    "             '20963086': 'Kylie',\n",
    "             '21235741': 'Jim',\n",
    "             '21235813': 'Nathan',\n",
    "             '21241679': 'Aaron',\n",
    "             '21349729': 'Sarah',\n",
    "             '21755924': 'Bob',\n",
    "             '22207515': 'Claire',\n",
    "             '22276941': 'Sally',\n",
    "             '23354087': 'Mike',\n",
    "             '24927292': 'Tim',\n",
    "             '26498711': 'Nina',\n",
    "             '26508082': 'Jacob',\n",
    "             '26514517': 'Lisa',\n",
    "             '26601370': 'Mitch',\n",
    "             '270093': 'Alan',\n",
    "             '270280': 'Tom',\n",
    "             '27380802': 'Jill',\n",
    "             '27546073': 'Walter',\n",
    "             '28068527': 'Simon',\n",
    "             '28222063': 'Annie',\n",
    "             '28880161': 'Chris',\n",
    "             '29652195': 'Nancy',\n",
    "             '29666163': 'Michael',\n",
    "             '29747722': 'Jason',\n",
    "             '29750591': 'Trey',\n",
    "             '29775461': 'Nicole',\n",
    "             '29775466': 'Liza',\n",
    "             '29818898': 'Thomas',\n",
    "             '29846726': 'Isaac',\n",
    "             '45033570': 'Patrick',\n",
    "             '46185459': 'Zo',\n",
    "             '51427894': 'Natalie'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan & 6.833 \\\\\n",
      "\n",
      "length: 1\n"
     ]
    }
   ],
   "source": [
    "######## results.ipynb ###########\n",
    "\n",
    "#save_results = open(\"save_results.txt\", \"a\")\n",
    "\n",
    "# print sorted list in latex table format\n",
    "for key_value_pair in sorted(average_depth.items(),\n",
    "           key=lambda k_v: k_v[1],\n",
    "           reverse=True):\n",
    "        \n",
    "        user = key_value_pair[0]\n",
    "        average = key_value_pair[1]\n",
    "        \n",
    "        print(\"{} & {:.3f} \\\\\\\\\".format(fake_name[user], average))\n",
    "        #save_results.write(fake_name[user] + \"\\t\" + str(average) + \"\\n\")\n",
    "\n",
    "print(\"\\nlength: \" + str(len(average_depth)))\n",
    "#save_results.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
