{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "punctlist = ['.', '!', '?', ',', ';', ':', '\"', \"'\", '(', ')', '--', '-', '...', '/', '\\'', '@',\n",
    "            '&', '>', '<', '=', '’', '^', '…', '_']\n",
    "stopwords = set(line.strip() for line in open('../stopwords.txt'))\n",
    "stopwords.update(['www', 'http', 'https', 'com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/noemi/Documents/USC/4-senior/2-spring2019/CSCE578/csce578final/groupme-fetch/transcript-15528094.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "id_to_name = defaultdict(str)\n",
    "doc_wordcounts = defaultdict(lambda: defaultdict(int)) # each document has a dict of word counts\n",
    "\n",
    "for item in data:\n",
    "    if item['text'] != None: \n",
    "        \n",
    "        user_id = item['user_id']\n",
    "        words = item['text']\n",
    "        \n",
    "        # ignore system and calendar messages\n",
    "        if user_id == 'system' or user_id == 'calendar':\n",
    "            continue\n",
    "            \n",
    "        # clean out punctuation\n",
    "        for punct in punctlist:\n",
    "            words = words.replace(punct, ' ')\n",
    "        words = words.lower().split()\n",
    "        \n",
    "        # take out stopwords\n",
    "        for w in words:\n",
    "            if w in stopwords:\n",
    "                # skip\n",
    "                continue\n",
    "            # add the words to the doc's word counts\n",
    "            doc_wordcounts[user_id][w] += 1\n",
    "        \n",
    "        # potentially add user to id_to_name dict\n",
    "        if user_id not in id_to_name:\n",
    "            id_to_name[user_id] = item['name']\n",
    "        \n",
    "        #print(\"%s: %s\" % (item['name'], item['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datetime\n",
    "#firstdate = data[0]['created_at']\n",
    "#datetime.datetime.fromtimestamp(int(firstdate)).strftime('%c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now turn these raw counts into frequencies (normalize by doc length)\n",
    "for doc in doc_wordcounts:\n",
    "    word_counts = doc_wordcounts[doc]\n",
    "    length = 0\n",
    "    for word in word_counts:\n",
    "        length += word_counts[word]\n",
    "    for word in word_counts:\n",
    "        word_counts[word] /= length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get idf for each term\n",
    "dfs_t = defaultdict(int)\n",
    "idfs_t = defaultdict(float)\n",
    "\n",
    "termlist = set()\n",
    "for doc in doc_wordcounts:\n",
    "    termlist.update(doc_wordcounts[doc].keys())\n",
    "    \n",
    "for term in termlist:\n",
    "    for doc in doc_wordcounts:\n",
    "        if term in doc_wordcounts[doc]:\n",
    "            dfs_t[term] += 1\n",
    "\n",
    "num_docs = len(doc_wordcounts)\n",
    "#print(num_docs)\n",
    "for term in termlist:\n",
    "    idfs_t[term] = math.log(num_docs/float(dfs_t[term]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idfs = defaultdict(lambda: defaultdict(float))\n",
    "for doc in doc_wordcounts:\n",
    "    for term in doc_wordcounts[doc]:\n",
    "        tf_idfs[doc][term] = doc_wordcounts[doc][term] * idfs_t[term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, in each document, sort terms by tf-idf\n",
    "# put top 10 into dict of vectors\n",
    "vectors = defaultdict(list)\n",
    "\n",
    "top_tfidf_words = set()\n",
    "for doc in tf_idfs:\n",
    "    #print('\\n**** user %s **** ' % id_to_name[doc])\n",
    "    \n",
    "    # top ten words per doc by tf-idf\n",
    "    for key_value_pair in sorted(tf_idfs[doc].items(),\n",
    "           key=lambda k_v: k_v[1],\n",
    "           reverse=True)[:10]:\n",
    "        \n",
    "        word = key_value_pair[0]\n",
    "        tf_idf = key_value_pair[1]\n",
    "        \n",
    "        top_tfidf_words.add(word)\n",
    "        \n",
    "        #print(\"%s:\\t%f\" % (word, tf_idf))\n",
    "    \n",
    "top_tfidf_words = list(top_tfidf_words)\n",
    "#print(len(top_tfidf_words))\n",
    "\n",
    "for doc in tf_idfs:\n",
    "    vec = []\n",
    "    for word in top_tfidf_words:\n",
    "        vec.append(tf_idfs[doc][word])\n",
    "    vectors[doc] = vec\n",
    "#print(len(vectors.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(arr1, arr2):\n",
    "    if len(arr1) != len(arr2):\n",
    "        print(\"vector dimensions don't match\")\n",
    "        return -1\n",
    "    \n",
    "    dot = 0\n",
    "    for i in range(len(arr1)):\n",
    "        dot += arr1[i]*arr2[i]\n",
    "    return dot\n",
    "        \n",
    "def norm(arr):\n",
    "    norm = 0\n",
    "    for i in range(len(arr)):\n",
    "        norm += math.pow(arr[i], 2)\n",
    "    return math.sqrt(float(norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make cluster matrix\n",
    "matrix = np.zeros((num_docs, num_docs), dtype=float)\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "# pprint(vectors.keys())\n",
    "# for doc in vectors:\n",
    "#     print(doc)\n",
    "# # check that they're in the same order\n",
    "\n",
    "for doc1 in vectors:\n",
    "    j=0\n",
    "    if i>37:\n",
    "        break\n",
    "    for doc2 in vectors:\n",
    "        if j>37:\n",
    "            break\n",
    "        \n",
    "        # populate with cos(alpha)\n",
    "        v1 = vectors[doc1]\n",
    "        v2 = vectors[doc2]\n",
    "        \n",
    "        num = (dot(v1, v2))\n",
    "        denom = norm(v1)*norm(v2)\n",
    "        if denom == 0: # one of the vectors is populated with 0s?\n",
    "            # then put the docs as far apart as possible (cos = 1)\n",
    "            matrix[i][j] = 1\n",
    "            continue\n",
    "        matrix[i][j] = num/denom\n",
    "        j += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_name = defaultdict(str)\n",
    "fake_name = {'10820250': 'Jack',\n",
    "             '13207027': 'John',\n",
    "             '14137183': 'Peter',\n",
    "             '18559389': 'Anna',\n",
    "             '18559680': 'Ethan',\n",
    "             '19458971': 'Alex',\n",
    "             '20963086': 'Kylie',\n",
    "             '21235741': 'Jim',\n",
    "             '21235813': 'Nathan',\n",
    "             '21241679': 'Aaron',\n",
    "             '21349729': 'Sarah',\n",
    "             '21755924': 'Bob',\n",
    "             '22207515': 'Claire',\n",
    "             '22276941': 'Sally',\n",
    "             '23354087': 'Mike',\n",
    "             '24927292': 'Tim',\n",
    "             '26498711': 'Nina',\n",
    "             '26508082': 'Jacob',\n",
    "             '26514517': 'Lisa',\n",
    "             '26601370': 'Mitch',\n",
    "             '270093': 'Alan',\n",
    "             '270280': 'tom',\n",
    "             '27380802': 'Jill',\n",
    "             '27546073': 'Walter',\n",
    "             '28068527': 'Simon',\n",
    "             '28222063': 'Annie',\n",
    "             '28880161': 'Chris',\n",
    "             '29652195': 'Nancy',\n",
    "             '29666163': 'Michael',\n",
    "             '29747722': 'Jason',\n",
    "             '29750591': 'Trey',\n",
    "             '29775461': 'Nicole',\n",
    "             '29775466': 'Liza',\n",
    "             '29818898': 'Thomas',\n",
    "             '29846726': 'Isaac',\n",
    "             '45033570': 'Patrick',\n",
    "             '46185459': 'Zo',\n",
    "             '51427894': 'Natalie'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_indices = list(vectors.keys())\n",
    "similarities = defaultdict(float)\n",
    "\n",
    "# only traverse half the matrix (since it's symmetric)\n",
    "for i in range(len(matrix)):\n",
    "    for j in range(i, len(matrix)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        doc_pair = (fake_name[doc_indices[i]], fake_name[doc_indices[j]])\n",
    "        similarities[doc_pair] = matrix[i][j]\n",
    "#         if matrix[i][j] > 0.5:\n",
    "#             print(\"%s and %s have a cosine of %s\" % (doc_indices[i], doc_indices[j], matrix[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top ten similar docs\n",
    "for key_value_pair in sorted(similarities.items(),\n",
    "           key=lambda k_v: k_v[1],\n",
    "           reverse=True)[:10]:\n",
    "    doc_pair = key_value_pair[0]\n",
    "    cosine = key_value_pair[1]\n",
    "    print(\"%s:\\t\\t%s\" % (doc_pair, cosine))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
